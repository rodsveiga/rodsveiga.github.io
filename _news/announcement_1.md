---
layout: post
date: 2023-02-12 15:59:00-0400
inline: true
related_posts: true
---

Gradient-based stochastic learning algorithms are the workhorses of modern machine learning. Understanding the role of stochasticity in the learning dynamics, as well as in achieving different global minima and how it is related to good generalization performances, is of most importance. Based on a continuous-time representation of stochastic gradient descent, in our new preprint [Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features](https://arxiv.org/abs/2402.07626) with Anastasia Remizova and Nicolas Macris, we construct a path-integral framework to compute the difference between pure gradient flow and stochastic gradient flow trajectories in the limit of small learning rate. We successfully apply the formalism to a high-dimensional linear regression model with random projections. Check it out!